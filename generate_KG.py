import tkinter as tk
from tkinter import messagebox
from tkinter import scrolledtext
import os
import webbrowser

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import math
import torch

# knowldege base class
from Knowledge_Base import KB
from Knowledge_Base_url import KB_url

# wrapper for wikipedia API
import wikipedia

# scraping of web articles
from newspaper import Article, ArticleException

# google news scraping
from GoogleNews import GoogleNews

# graph visualization
from pyvis.network import Network

# show HTML in notebook
import IPython

# Load the pre-trained REBEL model and tokenizer.
tokenizer = AutoTokenizer.from_pretrained("Babelscape/rebel-large")
model = AutoModelForSeq2SeqLM.from_pretrained("Babelscape/rebel-large", trust_remote_code=True)

# A function that is able to parse the strings generated by REBEL and transform them into relation triples.
# from https://huggingface.co/Babelscape/rebel-large
def extract_relations_from_model_output(text):
    relations = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    text_replaced = text.replace("<s>", "").replace("<pad>", "").replace("</s>", "")
    for token in text_replaced.split():
        if token == "<triplet>":
            current = 't'
            if relation != '':
                relations.append({
                    'head': subject.strip(),
                    'type': relation.strip(),
                    'tail': object_.strip()
                })
                relation = ''
            subject = ''
        elif token == "<subj>":
            current = 's'
            if relation != '':
                relations.append({
                    'head': subject.strip(),
                    'type': relation.strip(),
                    'tail': object_.strip()
                })
            object_ = ''
        elif token == "<obj>":
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        relations.append({
            'head': subject.strip(),
            'type': relation.strip(),
            'tail': object_.strip()
        })
    return relations

# extract relations for each span and put them together in a knowledge base
def from_text_to_kb(text, span_length=128, verbose=False):
    # tokenize whole text
    inputs = tokenizer([text], return_tensors="pt")

    # compute span boundaries
    num_tokens = len(inputs["input_ids"][0])
    if verbose:
        print(f"Input has {num_tokens} tokens")
    num_spans = math.ceil(num_tokens / span_length)
    if verbose:
        print(f"Input has {num_spans} spans")
    overlap = math.ceil((num_spans * span_length - num_tokens) /
                        max(num_spans - 1, 1))
    spans_boundaries = []
    start = 0
    for i in range(num_spans):
        spans_boundaries.append([start + span_length * i,
                                 start + span_length * (i + 1)])
        start -= overlap
    if verbose:
        print(f"Span boundaries are {spans_boundaries}")

    # transform input with spans
    tensor_ids = [inputs["input_ids"][0][boundary[0]:boundary[1]]
                  for boundary in spans_boundaries]
    tensor_masks = [inputs["attention_mask"][0][boundary[0]:boundary[1]]
                    for boundary in spans_boundaries]
    inputs = {
        "input_ids": torch.stack(tensor_ids),
        "attention_mask": torch.stack(tensor_masks)
    }

    # generate relations
    num_return_sequences = 3
    gen_kwargs = {
        "max_length": 256,
        "length_penalty": 0,
        "num_beams": 3,
        "num_return_sequences": num_return_sequences
    }
    generated_tokens = model.generate(
        **inputs,
        **gen_kwargs,
    )

    # decode relations
    decoded_preds = tokenizer.batch_decode(generated_tokens,
                                           skip_special_tokens=False)

    # create kb
    kb = KB()
    i = 0
    for sentence_pred in decoded_preds:
        current_span_index = i // num_return_sequences
        relations = extract_relations_from_model_output(sentence_pred)
        for relation in relations:
            relation["meta"] = {
                "spans": [spans_boundaries[current_span_index]]
            }
            kb.add_relation(relation)
        i += 1

    return kb

# extract text from url, extract relations and populate the KB
def from_text_to_kb_url(text, article_url, span_length=128, article_title=None,
                    article_publish_date=None, verbose=False):
  # tokenize whole text
    inputs = tokenizer([text], return_tensors="pt")

    # compute span boundaries
    num_tokens = len(inputs["input_ids"][0])
    if verbose:
        print(f"Input has {num_tokens} tokens")
    num_spans = math.ceil(num_tokens / span_length)
    if verbose:
        print(f"Input has {num_spans} spans")
    overlap = math.ceil((num_spans * span_length - num_tokens) /
                        max(num_spans - 1, 1))
    spans_boundaries = []
    start = 0
    for i in range(num_spans):
        spans_boundaries.append([start + span_length * i,
                                 start + span_length * (i + 1)])
        start -= overlap
    if verbose:
        print(f"Span boundaries are {spans_boundaries}")

    # transform input with spans
    tensor_ids = [inputs["input_ids"][0][boundary[0]:boundary[1]]
                  for boundary in spans_boundaries]
    tensor_masks = [inputs["attention_mask"][0][boundary[0]:boundary[1]]
                    for boundary in spans_boundaries]
    inputs = {
        "input_ids": torch.stack(tensor_ids),
        "attention_mask": torch.stack(tensor_masks)
    }

    # generate relations
    num_return_sequences = 3
    gen_kwargs = {
        "max_length": 256,
        "length_penalty": 0,
        "num_beams": 3,
        "num_return_sequences": num_return_sequences
    }
    generated_tokens = model.generate(
        **inputs,
        **gen_kwargs,
    )

    # decode relations
    decoded_preds = tokenizer.batch_decode(generated_tokens,
                                           skip_special_tokens=False)
    # create kb
    kb = KB_url()
    i = 0
    for sentence_pred in decoded_preds:
        current_span_index = i // num_return_sequences
        relations = extract_relations_from_model_output(sentence_pred)
        for relation in relations:
            relation["meta"] = {
                article_url: {
                    "spans": [spans_boundaries[current_span_index]]
                }
            }
            kb.add_relation(relation, article_title, article_publish_date)
        i += 1

    return kb

def get_article(url):
  article = Article(url)
  article.download()
  article.parse()
  return article

def from_url_to_kb(url):
  article = get_article(url)
  config = {
      "article_title" : article.title,
      "article_publish_date" : article.publish_date
  }
  kb = from_text_to_kb_url(article.text, article.url, **config)
  return kb

# build a KB from multiple news links
def from_urls_to_kb(urls, verbose=False):
    kb = KB_url()
    if verbose:
        print(f"{len(urls)} links to visit")
    for url in urls:
        if verbose:
            print(f"Visiting {url}...")
        try:
            kb_url = from_url_to_kb(url)
            kb.merge_with_kb(kb_url)
        except ArticleException:
            if verbose:
                print(f"  Couldn't download article at url {url}")
    return kb

# from KB to HTML visualization
def save_network_html(kb, filename="network.html"):
    # create network
    net = Network(directed=True, width="auto", height="700px", bgcolor="#eeeeee")

    # nodes
    color_entity = "#00FF00"
    for e in kb.entities:
        net.add_node(e, shape="circle", color=color_entity)

    # edges
    for r in kb.relations:
        net.add_edge(r["head"], r["tail"],
                    title=r["type"], label=r["type"])

    # save network
    net.repulsion(
        node_distance=200,
        central_gravity=0.2,
        spring_length=200,
        spring_strength=0.05,
        damping=0.09
    )
    net.set_edge_smooth('dynamic')
    net.show(filename)

# Function to handle the process when the user clicks the "Generate Graph" button.
def on_generate_graph_click():
    # Get the user input text
    user_text = text_input.get("1.0", tk.END).strip()

    if not user_text:
        messagebox.showerror("Error", "Please enter some text.")
        return

    try:
        # Generate the knowledge base
        kb = from_text_to_kb(user_text, verbose=True)

        # Save the graph to an HTML file
        filename = "network_from_input.html"
        save_network_html(kb, filename=filename)

        # Display the graph in the default web browser
        webbrowser.open(f"file://{os.path.abspath(filename)}")

    except Exception as e:
        messagebox.showerror("Error", f"An error occurred while generating the graph: {e}")

def on_generate_graph_click_url():
    # Get the user input URLs
    user_urls = url_input.get("1.0", tk.END).strip().splitlines()

    if not user_urls:
        messagebox.showerror("Error", "Please enter at least one URL.")
        return

    try:
        # Generate the knowledge base
        kb = from_urls_to_kb(user_urls, verbose=True)

        # Save the graph to an HTML file
        filename = "network_from_urls.html"
        save_network_html(kb, filename=filename)

        # Display the graph in the default web browser
        webbrowser.open(f"file://{os.path.abspath(filename)}")

    except Exception as e:
        messagebox.showerror("Error", f"An error occurred while generating the graph: {e}")



if __name__ == '__main__':

    choice = input("How do you want to make KG? From URL or Text: ")
    if(choice == "text"):
        root = tk.Tk()
        root.title("Text to Knowledge Graph Generator")
        root.geometry("800x600")  # You can adjust the window size as needed

        # Label for instructions
        label = tk.Label(root, text="Enter text to generate a knowledge graph:", font=("Arial", 14))
        label.pack(pady=10)

        # Text input area (scrollable)
        # text_input = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=80, height=10, font=("Arial", 12))
        text_input = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=80, height=22, font=("Arial", 12))

        text_input.pack(pady=10)

        # Button to trigger graph generation
        generate_button = tk.Button(root, text="Generate Graph", font=("Arial", 14), command=on_generate_graph_click)
        generate_button.pack(pady=20)

        # Start the tkinter event loop
        root.mainloop()

    if(choice == "url"):
        # For testing: "https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html","https://finance.yahoo.com/news/brazils-energy-minister-asks-petrobras-201522428.html"
        root = tk.Tk()
        root.title("URL to Knowledge Graph Generator")
        root.geometry("800x600")

        label = tk.Label(root, text="Enter URLs (one per line):", font=("Arial", 14))
        label.pack(pady=10)

        url_input = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=80, height=22, font=("Arial", 12))
        url_input.pack(pady=10)

        generate_button = tk.Button(root, text="Generate Graph", font=("Arial", 14), command=on_generate_graph_click_url)
        generate_button.pack(pady=20)

        root.mainloop()
